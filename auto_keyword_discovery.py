"""
ë„¤ì´ë²„ ì‡¼í•‘ ìë™ í‚¤ì›Œë“œ ë°œê²¬ ì‹œìŠ¤í…œ
ì‹¤ì‹œê°„ ì¸ê¸° ì œí’ˆì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
"""

import requests
import urllib3
from typing import List, Dict, Set
import json
from pathlib import Path
import re
from collections import Counter

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


def naver_shopping_search(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 100,
    sort: str = "sim"
) -> Dict:
    """
    ë„¤ì´ë²„ ì‡¼í•‘ ê²€ìƒ‰ API
    
    Args:
        query: ê²€ìƒ‰ì–´
        client_id: API Client ID
        client_secret: API Client Secret
        display: ê²°ê³¼ ê°œìˆ˜ (ìµœëŒ€ 100)
        sort: ì •ë ¬ (sim:ìœ ì‚¬ë„, date:ë‚ ì§œ, asc:ê°€ê²©ì˜¤ë¦„ì°¨ìˆœ, dsc:ê°€ê²©ë‚´ë¦¼ì°¨ìˆœ)
    
    Returns:
        API ì‘ë‹µ
    """
    url = "https://openapi.naver.com/v1/search/shop.json"
    
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    
    params = {
        "query": query,
        "display": display,
        "sort": sort
    }
    
    try:
        response = requests.get(
            url,
            headers=headers,
            params=params,
            timeout=30,
            verify=False
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨ ({query}): {str(e)}")
        return {"items": []}


def extract_keywords_from_products(products: List[Dict], min_freq: int = 2) -> List[str]:
    """
    ì œí’ˆ ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    Args:
        products: ì œí’ˆ ëª©ë¡
        min_freq: ìµœì†Œ ë¹ˆë„ (ì´ íšŸìˆ˜ ì´ìƒ ë“±ì¥í•œ ë‹¨ì–´ë§Œ)
    
    Returns:
        í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    keywords = []
    brands = set()
    
    for product in products:
        title = product.get("title", "")
        brand = product.get("brand", "")
        
        # HTML íƒœê·¸ ì œê±°
        title = re.sub(r'<[^>]+>', '', title)
        brand = re.sub(r'<[^>]+>', '', brand)
        
        # ë¸Œëœë“œëª… ìˆ˜ì§‘
        if brand and len(brand) >= 2:
            brands.add(brand.strip())
        
        # ì œëª©ì—ì„œ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ
        # í•œê¸€ 2ì ì´ìƒ, ì˜ë¬¸ 3ì ì´ìƒ
        words = re.findall(r'[ê°€-í£]{2,}|[A-Za-z]{3,}', title)
        keywords.extend(words)
    
    # ë¹ˆë„ ë¶„ì„
    word_freq = Counter(keywords)
    
    # ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ + ëª¨ë“  ë¸Œëœë“œ
    frequent_words = [word for word, freq in word_freq.items() if freq >= min_freq]
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    all_keywords = list(set(frequent_words) | brands)
    
    return sorted(all_keywords)


def discover_trending_keywords_hierarchical(
    client_id: str,
    client_secret: str,
    categories: Dict,
    max_keywords_per_category: int = 30
) -> Dict:
    """
    ê³„ì¸µì  ì¹´í…Œê³ ë¦¬ë³„ íŠ¸ë Œë”© í‚¤ì›Œë“œ ìë™ ë°œê²¬
    
    Args:
        client_id: API Client ID
        client_secret: API Client Secret
        categories: ê³„ì¸µì  ì¹´í…Œê³ ë¦¬ êµ¬ì¡°
        max_keywords_per_category: ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
    
    Returns:
        ê³„ì¸µì  êµ¬ì¡°ì˜ í‚¤ì›Œë“œ
    """
    from category_manager import CategoryManager
    
    manager = CategoryManager()
    
    print("="*70)
    print("ğŸ” ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìë™ ë°œê²¬ (ê³„ì¸µì )")
    print("="*70)
    
    for major_category, cat_data in categories.items():
        print(f"\nğŸ“¦ {major_category}")
        
        # ëŒ€ë¶„ë¥˜ í‚¤ì›Œë“œ ìˆ˜ì§‘
        if "ëŒ€ë¶„ë¥˜" in cat_data:
            print(f"  ğŸ¢ ëŒ€ë¶„ë¥˜ í‚¤ì›Œë“œ ìˆ˜ì§‘...")
            all_products = []
            
            for query in cat_data["ëŒ€ë¶„ë¥˜"]:
                print(f"    ê²€ìƒ‰: {query}...", end=" ")
                data = naver_shopping_search(
                    query=query,
                    client_id=client_id,
                    client_secret=client_secret,
                    display=100,
                    sort="sim"
                )
                items = data.get("items", [])
                all_products.extend(items)
                print(f"âœ“ {len(items)}ê°œ")
            
            keywords = extract_keywords_from_products(all_products, min_freq=3)
            keywords = keywords[:max_keywords_per_category]
            
            # ëŒ€ë¶„ë¥˜ì— ì €ì¥
            manager.update_auto_keywords(major_category, keywords, sub=None)
            
            print(f"    âœ… ëŒ€ë¶„ë¥˜: {len(keywords)}ê°œ í‚¤ì›Œë“œ")
            print(f"       ì˜ˆ: {', '.join(keywords[:5])}")
        
        # ì¤‘ë¶„ë¥˜ í‚¤ì›Œë“œ ìˆ˜ì§‘
        if "ì¤‘ë¶„ë¥˜" in cat_data and cat_data["ì¤‘ë¶„ë¥˜"]:
            print(f"  ğŸ“ ì¤‘ë¶„ë¥˜ í‚¤ì›Œë“œ ìˆ˜ì§‘...")
            
            for sub_category, search_queries in cat_data["ì¤‘ë¶„ë¥˜"].items():
                print(f"    â””â”€ {sub_category}:", end=" ")
                
                all_products = []
                for query in search_queries:
                    data = naver_shopping_search(
                        query=query,
                        client_id=client_id,
                        client_secret=client_secret,
                        display=50,
                        sort="sim"
                    )
                    items = data.get("items", [])
                    all_products.extend(items)
                
                keywords = extract_keywords_from_products(all_products, min_freq=2)
                keywords = keywords[:max_keywords_per_category]
                
                # ì¤‘ë¶„ë¥˜ì— ì €ì¥
                manager.update_auto_keywords(major_category, keywords, sub=sub_category)
                
                print(f" âœ“ {len(keywords)}ê°œ ({', '.join(keywords[:3])}...)")
    
    print(f"\n{'='*70}")
    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"{'='*70}")
    
    return manager.data


def discover_trending_keywords(
    client_id: str,
    client_secret: str,
    categories: Dict[str, List[str]],
    max_keywords_per_category: int = 30
) -> Dict[str, List[str]]:
    """
    ì¹´í…Œê³ ë¦¬ë³„ íŠ¸ë Œë”© í‚¤ì›Œë“œ ìë™ ë°œê²¬ (í•˜ìœ„ í˜¸í™˜ì„±)
    
    Args:
        client_id: API Client ID
        client_secret: API Client Secret
        categories: {ì¹´í…Œê³ ë¦¬ëª…: [ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸]}
        max_keywords_per_category: ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
    
    Returns:
        {ì¹´í…Œê³ ë¦¬ëª…: [ìë™ ë°œê²¬ëœ í‚¤ì›Œë“œ]}
    """
    discovered = {}
    
    print("="*70)
    print("ğŸ” ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìë™ ë°œê²¬")
    print("="*70)
    
    for category, search_queries in categories.items():
        print(f"\nğŸ“¦ {category}")
        
        all_products = []
        
        # ê° ê²€ìƒ‰ì–´ë¡œ ì œí’ˆ ìˆ˜ì§‘
        for query in search_queries:
            print(f"  ê²€ìƒ‰: {query}...", end=" ")
            
            data = naver_shopping_search(
                query=query,
                client_id=client_id,
                client_secret=client_secret,
                display=100,
                sort="sim"  # ì¸ê¸°ìˆœ
            )
            
            items = data.get("items", [])
            all_products.extend(items)
            print(f"âœ“ {len(items)}ê°œ")
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords_from_products(all_products, min_freq=3)
        
        # ìƒìœ„ Nê°œë§Œ
        keywords = keywords[:max_keywords_per_category]
        
        discovered[category] = keywords
        
        print(f"  âœ… ë°œê²¬: {len(keywords)}ê°œ í‚¤ì›Œë“œ")
        print(f"     ì˜ˆ: {', '.join(keywords[:5])}")
    
    return discovered


def merge_with_existing_keywords(
    discovered: Dict[str, List[str]],
    existing_file: str = "./naver_categories.json",
    mode: str = "replace"
) -> Dict[str, List[str]]:
    """
    ê¸°ì¡´ í‚¤ì›Œë“œì™€ ë³‘í•©
    
    Args:
        discovered: ìƒˆë¡œ ë°œê²¬ëœ í‚¤ì›Œë“œ
        existing_file: ê¸°ì¡´ í‚¤ì›Œë“œ íŒŒì¼
        mode: "merge" (ëˆ„ì ) ë˜ëŠ” "replace" (êµì²´, ê¸°ë³¸ê°’)
    
    Returns:
        ë³‘í•©ëœ í‚¤ì›Œë“œ
    """
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    existing = {}
    existing_path = Path(existing_file)
    
    if existing_path.exists():
        try:
            with open(existing_path, "r", encoding="utf-8") as f:
                existing = json.load(f)
            print(f"\nğŸ“¦ ê¸°ì¡´ ë°ì´í„°: {len(existing)}ê°œ ì¹´í…Œê³ ë¦¬, {sum(len(v) for v in existing.values())}ê°œ í‚¤ì›Œë“œ")
        except:
            pass
    
    merged = {}
    
    if mode == "merge":
        # ë³‘í•© ëª¨ë“œ: ê¸°ì¡´ + ì‹ ê·œ (ëˆ„ì )
        print("ğŸ”„ ëª¨ë“œ: ë³‘í•© (ê¸°ì¡´ í‚¤ì›Œë“œ ìœ ì§€ + ì‹ ê·œ ì¶”ê°€)")
        
        for category in set(list(existing.keys()) + list(discovered.keys())):
            old_keywords = set(existing.get(category, []))
            new_keywords = set(discovered.get(category, []))
            
            # í•©ì¹˜ê¸°
            merged[category] = sorted(list(old_keywords | new_keywords))
    
    else:  # mode == "replace"
        # êµì²´ ëª¨ë“œ: ì‹ ê·œë¡œë§Œ êµì²´ (ìµœì‹  ì¸ê¸° í‚¤ì›Œë“œë§Œ ìœ ì§€)
        print("ğŸ”„ ëª¨ë“œ: êµì²´ (ìµœì‹  ì¸ê¸° í‚¤ì›Œë“œë§Œ ìœ ì§€)")
        merged = discovered
    
    print(f"âœ… ê²°ê³¼: {len(merged)}ê°œ ì¹´í…Œê³ ë¦¬, {sum(len(v) for v in merged.values())}ê°œ í‚¤ì›Œë“œ")
    
    return merged


def save_keywords(
    keywords: Dict[str, List[str]],
    save_path: str = "./naver_categories.json",
    backup: bool = True
) -> None:
    """
    í‚¤ì›Œë“œ ì €ì¥
    
    Args:
        keywords: í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬
        save_path: ì €ì¥ ê²½ë¡œ
        backup: ë°±ì—… ìƒì„± ì—¬ë¶€
    """
    save_path = Path(save_path)
    
    # ë°±ì—… ìƒì„±
    if backup and save_path.exists():
        import shutil
        from datetime import datetime
        backup_name = f"{save_path.stem}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        backup_path = save_path.parent / backup_name
        shutil.copy(save_path, backup_path)
        print(f"ğŸ“¦ ë°±ì—…: {backup_path}")
    
    # ì €ì¥
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(keywords, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì €ì¥: {save_path}")
    print(f"   ì´ {len(keywords)}ê°œ ì¹´í…Œê³ ë¦¬, {sum(len(v) for v in keywords.values())}ê°œ í‚¤ì›Œë“œ")


# ì¹´í…Œê³ ë¦¬ë³„ ì‹œë“œ ê²€ìƒ‰ì–´ (ê³„ì¸µì  êµ¬ì¡°)
SEED_QUERIES = {
    "íŒ¨ì…˜ì˜ë¥˜": {
        "ëŒ€ë¶„ë¥˜": ["íŒ¨ì…˜ì˜ë¥˜", "ì˜ë¥˜"],
        "ì¤‘ë¶„ë¥˜": {
            "ì—¬ì„±ì˜ë¥˜": ["ì—¬ì„±ì˜ë¥˜", "ì—¬ì„±ì˜·", "ë ˆì´ë””ìŠ¤"],
            "ë‚¨ì„±ì˜ë¥˜": ["ë‚¨ì„±ì˜ë¥˜", "ë‚¨ì„±ì˜·", "ë§¨ì¦ˆ"],
            "ì–¸ë”ì›¨ì–´": ["ì†ì˜·", "ì´ë„ˆì›¨ì–´", "ì–¸ë”ì›¨ì–´"]
        }
    },
    "íŒ¨ì…˜ì¡í™”": {
        "ëŒ€ë¶„ë¥˜": ["íŒ¨ì…˜ì¡í™”", "ì•¡ì„¸ì„œë¦¬"],
        "ì¤‘ë¶„ë¥˜": {
            "ì—¬ì„±ê°€ë°©": ["ì—¬ì„±ê°€ë°©", "ìˆ„ë”ë°±", "í¬ë¡œìŠ¤ë°±"],
            "ë‚¨ì„±ê°€ë°©": ["ë‚¨ì„±ê°€ë°©", "ë°±íŒ©", "ë¹„ì¦ˆë‹ˆìŠ¤ë°±"],
            "ì§€ê°‘": ["ì§€ê°‘", "ë°˜ì§€ê°‘", "ì¥ì§€ê°‘"]
        }
    },
    "í™”ì¥í’ˆ/ë¯¸ìš©": {
        "ëŒ€ë¶„ë¥˜": ["í™”ì¥í’ˆ", "ì½”ìŠ¤ë©”í‹±"],
        "ì¤‘ë¶„ë¥˜": {
            "ìŠ¤í‚¨ì¼€ì–´": ["ìŠ¤í‚¨ì¼€ì–´", "ë¡œì…˜", "í¬ë¦¼", "ì—ì„¼ìŠ¤"],
            "ë©”ì´í¬ì—…": ["ë©”ì´í¬ì—…", "ë¦½ìŠ¤í‹±", "íŒŒìš´ë°ì´ì…˜"],
            "í–¥ìˆ˜": ["í–¥ìˆ˜", "í¼í“¸", "í”„ë˜ê·¸ëŸ°ìŠ¤"]
        }
    },
    "ë””ì§€í„¸/ê°€ì „": {
        "ëŒ€ë¶„ë¥˜": ["ë””ì§€í„¸", "ì „ìê¸°ê¸°"],
        "ì¤‘ë¶„ë¥˜": {}
    },
    "ì‹í’ˆ": {
        "ëŒ€ë¶„ë¥˜": ["ì‹í’ˆ", "ê±´ê°•ì‹í’ˆ"],
        "ì¤‘ë¶„ë¥˜": {
            "ë†ìˆ˜ì¶•ì‚°ë¬¼": ["ë†ì‚°ë¬¼", "ìˆ˜ì‚°ë¬¼", "ì¶•ì‚°ë¬¼"],
            "ê°€ê³µì‹í’ˆ": ["ê°€ê³µì‹í’ˆ", "ì¦‰ì„ì‹í’ˆ"],
            "ê±´ê°•ì‹í’ˆ": ["ê±´ê°•ì‹í’ˆ", "ì˜ì–‘ì œ", "ë¹„íƒ€ë¯¼"]
        }
    },
    "ìƒí™œ/ê±´ê°•": {
        "ëŒ€ë¶„ë¥˜": ["ìƒí™œê±´ê°•", "ê±´ê°•ìš©í’ˆ"],
        "ì¤‘ë¶„ë¥˜": {
            "ìƒí™œìš©í’ˆ": ["ìƒí™œìš©í’ˆ", "ì£¼ë°©ìš©í’ˆ"],
            "ê±´ê°•ìš©í’ˆ": ["ê±´ê°•ìš©í’ˆ", "ì˜ë£Œìš©í’ˆ"],
            "ì˜ë£Œìš©í’ˆ": ["ì˜ë£Œìš©í’ˆ", "êµ¬ê¸‰ìš©í’ˆ"]
        }
    },
    "ì¶œì‚°/ìœ¡ì•„": {
        "ëŒ€ë¶„ë¥˜": ["ì¶œì‚°ìœ¡ì•„", "ìœ¡ì•„ìš©í’ˆ"],
        "ì¤‘ë¶„ë¥˜": {
            "ê¸°ì €ê·€": ["ê¸°ì €ê·€", "ìœ ì•„ìš©ê¸°ì €ê·€"],
            "ë¶„ìœ ": ["ë¶„ìœ ", "ìœ ì•„ì‹", "ì´ìœ ì‹"],
            "ì´ìœ ì‹": ["ì´ìœ ì‹", "ìœ ì•„ì‹"]
        }
    },
    "ìŠ¤í¬ì¸ /ë ˆì €": {
        "ëŒ€ë¶„ë¥˜": ["ìŠ¤í¬ì¸ ", "ë ˆì €ìš©í’ˆ"],
        "ì¤‘ë¶„ë¥˜": {}
    },
    "ì—¬ê°€/ìƒí™œí¸ì˜": {
        "ëŒ€ë¶„ë¥˜": ["ìƒí™œí¸ì˜", "ì—¬ê°€ìš©í’ˆ"],
        "ì¤‘ë¶„ë¥˜": {}
    }
}


if __name__ == "__main__":
    import os
    
    CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "9LKTOG5R9F8Yx74PnZe0")
    CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET", "gytCGuuEeX")
    
    print("ğŸš€ ìë™ í‚¤ì›Œë“œ ë°œê²¬ ì‹œì‘\n")
    
    # 1. íŠ¸ë Œë”© í‚¤ì›Œë“œ ë°œê²¬
    discovered = discover_trending_keywords(
        client_id=CLIENT_ID,
        client_secret=CLIENT_SECRET,
        categories=SEED_QUERIES,
        max_keywords_per_category=50
    )
    
    print(f"\n{'='*70}")
    print("ğŸ“Š ë°œê²¬ ê²°ê³¼")
    print(f"{'='*70}")
    
    for category, keywords in discovered.items():
        print(f"\n{category}: {len(keywords)}ê°œ")
        print(f"  {', '.join(keywords[:10])}")
        if len(keywords) > 10:
            print(f"  ... ì™¸ {len(keywords)-10}ê°œ")
    
    # 2. ê¸°ì¡´ í‚¤ì›Œë“œì™€ ë³‘í•©
    print(f"\n{'='*70}")
    print("ğŸ”„ ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©")
    print(f"{'='*70}")
    
    # ê¸°ë³¸ê°’: replace (ìµœì‹  ì¸ê¸° í‚¤ì›Œë“œë§Œ ìœ ì§€)
    # "merge"ë¡œ ë³€ê²½í•˜ë©´ ê¸°ì¡´ í‚¤ì›Œë“œë„ ìœ ì§€
    merged = merge_with_existing_keywords(discovered, mode="replace")
    
    # 3. ì €ì¥
    print(f"\n{'='*70}")
    print("ğŸ’¾ ì €ì¥")
    print(f"{'='*70}\n")
    
    save_keywords(merged, backup=True)
    
    print(f"\n{'='*70}")
    print("âœ¨ ì™„ë£Œ!")
    print(f"{'='*70}")
    print("\nğŸ’¡ ì´ì œ Streamlit ì•±ì„ ë‹¤ì‹œ ì‹œì‘í•˜ë©´ ìƒˆë¡œìš´ í‚¤ì›Œë“œë¡œ ë¶„ì„ë©ë‹ˆë‹¤!")
    print("   streamlit run streamlit_app.py")

