"""
ë„¤ì´ë²„ ì‡¼í•‘ ìë™ í‚¤ì›Œë“œ ë°œê²¬ ì‹œìŠ¤í…œ
ì‹¤ì‹œê°„ ì¸ê¸° ì œí’ˆì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
"""

import requests
import urllib3
from typing import List, Dict, Set
import json
from pathlib import Path
import re
from collections import Counter

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


def naver_shopping_search(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 100,
    sort: str = "sim"
) -> Dict:
    """
    ë„¤ì´ë²„ ì‡¼í•‘ ê²€ìƒ‰ API
    
    Args:
        query: ê²€ìƒ‰ì–´
        client_id: API Client ID
        client_secret: API Client Secret
        display: ê²°ê³¼ ê°œìˆ˜ (ìµœëŒ€ 100)
        sort: ì •ë ¬ (sim:ìœ ì‚¬ë„, date:ë‚ ì§œ, asc:ê°€ê²©ì˜¤ë¦„ì°¨ìˆœ, dsc:ê°€ê²©ë‚´ë¦¼ì°¨ìˆœ)
    
    Returns:
        API ì‘ë‹µ
    """
    url = "https://openapi.naver.com/v1/search/shop.json"
    
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    
    params = {
        "query": query,
        "display": display,
        "sort": sort
    }
    
    try:
        response = requests.get(
            url,
            headers=headers,
            params=params,
            timeout=30,
            verify=False
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨ ({query}): {str(e)}")
        return {"items": []}


def extract_keywords_from_products(products: List[Dict], min_freq: int = 2) -> List[str]:
    """
    ì œí’ˆ ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    Args:
        products: ì œí’ˆ ëª©ë¡
        min_freq: ìµœì†Œ ë¹ˆë„ (ì´ íšŸìˆ˜ ì´ìƒ ë“±ì¥í•œ ë‹¨ì–´ë§Œ)
    
    Returns:
        í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    keywords = []
    brands = set()
    
    for product in products:
        title = product.get("title", "")
        brand = product.get("brand", "")
        
        # HTML íƒœê·¸ ì œê±°
        title = re.sub(r'<[^>]+>', '', title)
        brand = re.sub(r'<[^>]+>', '', brand)
        
        # ë¸Œëœë“œëª… ìˆ˜ì§‘
        if brand and len(brand) >= 2:
            brands.add(brand.strip())
        
        # ì œëª©ì—ì„œ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ
        # í•œê¸€ 2ì ì´ìƒ, ì˜ë¬¸ 3ì ì´ìƒ
        words = re.findall(r'[ê°€-í£]{2,}|[A-Za-z]{3,}', title)
        keywords.extend(words)
    
    # ë¹ˆë„ ë¶„ì„
    word_freq = Counter(keywords)
    
    # ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ + ëª¨ë“  ë¸Œëœë“œ
    frequent_words = [word for word, freq in word_freq.items() if freq >= min_freq]
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    all_keywords = list(set(frequent_words) | brands)
    
    return sorted(all_keywords)


def discover_trending_keywords(
    client_id: str,
    client_secret: str,
    categories: Dict[str, List[str]],
    max_keywords_per_category: int = 30
) -> Dict[str, List[str]]:
    """
    ì¹´í…Œê³ ë¦¬ë³„ íŠ¸ë Œë”© í‚¤ì›Œë“œ ìë™ ë°œê²¬
    
    Args:
        client_id: API Client ID
        client_secret: API Client Secret
        categories: {ì¹´í…Œê³ ë¦¬ëª…: [ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸]}
        max_keywords_per_category: ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
    
    Returns:
        {ì¹´í…Œê³ ë¦¬ëª…: [ìë™ ë°œê²¬ëœ í‚¤ì›Œë“œ]}
    """
    discovered = {}
    
    print("="*70)
    print("ğŸ” ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìë™ ë°œê²¬")
    print("="*70)
    
    for category, search_queries in categories.items():
        print(f"\nğŸ“¦ {category}")
        
        all_products = []
        
        # ê° ê²€ìƒ‰ì–´ë¡œ ì œí’ˆ ìˆ˜ì§‘
        for query in search_queries:
            print(f"  ê²€ìƒ‰: {query}...", end=" ")
            
            data = naver_shopping_search(
                query=query,
                client_id=client_id,
                client_secret=client_secret,
                display=100,
                sort="sim"  # ì¸ê¸°ìˆœ
            )
            
            items = data.get("items", [])
            all_products.extend(items)
            print(f"âœ“ {len(items)}ê°œ")
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords_from_products(all_products, min_freq=3)
        
        # ìƒìœ„ Nê°œë§Œ
        keywords = keywords[:max_keywords_per_category]
        
        discovered[category] = keywords
        
        print(f"  âœ… ë°œê²¬: {len(keywords)}ê°œ í‚¤ì›Œë“œ")
        print(f"     ì˜ˆ: {', '.join(keywords[:5])}")
    
    return discovered


def merge_with_existing_keywords(
    discovered: Dict[str, List[str]],
    existing_file: str = "./naver_categories.json",
    mode: str = "replace"
) -> Dict[str, List[str]]:
    """
    ê¸°ì¡´ í‚¤ì›Œë“œì™€ ë³‘í•©
    
    Args:
        discovered: ìƒˆë¡œ ë°œê²¬ëœ í‚¤ì›Œë“œ
        existing_file: ê¸°ì¡´ í‚¤ì›Œë“œ íŒŒì¼
        mode: "merge" (ëˆ„ì ) ë˜ëŠ” "replace" (êµì²´, ê¸°ë³¸ê°’)
    
    Returns:
        ë³‘í•©ëœ í‚¤ì›Œë“œ
    """
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    existing = {}
    existing_path = Path(existing_file)
    
    if existing_path.exists():
        try:
            with open(existing_path, "r", encoding="utf-8") as f:
                existing = json.load(f)
            print(f"\nğŸ“¦ ê¸°ì¡´ ë°ì´í„°: {len(existing)}ê°œ ì¹´í…Œê³ ë¦¬, {sum(len(v) for v in existing.values())}ê°œ í‚¤ì›Œë“œ")
        except:
            pass
    
    merged = {}
    
    if mode == "merge":
        # ë³‘í•© ëª¨ë“œ: ê¸°ì¡´ + ì‹ ê·œ (ëˆ„ì )
        print("ğŸ”„ ëª¨ë“œ: ë³‘í•© (ê¸°ì¡´ í‚¤ì›Œë“œ ìœ ì§€ + ì‹ ê·œ ì¶”ê°€)")
        
        for category in set(list(existing.keys()) + list(discovered.keys())):
            old_keywords = set(existing.get(category, []))
            new_keywords = set(discovered.get(category, []))
            
            # í•©ì¹˜ê¸°
            merged[category] = sorted(list(old_keywords | new_keywords))
    
    else:  # mode == "replace"
        # êµì²´ ëª¨ë“œ: ì‹ ê·œë¡œë§Œ êµì²´ (ìµœì‹  ì¸ê¸° í‚¤ì›Œë“œë§Œ ìœ ì§€)
        print("ğŸ”„ ëª¨ë“œ: êµì²´ (ìµœì‹  ì¸ê¸° í‚¤ì›Œë“œë§Œ ìœ ì§€)")
        merged = discovered
    
    print(f"âœ… ê²°ê³¼: {len(merged)}ê°œ ì¹´í…Œê³ ë¦¬, {sum(len(v) for v in merged.values())}ê°œ í‚¤ì›Œë“œ")
    
    return merged


def save_keywords(
    keywords: Dict[str, List[str]],
    save_path: str = "./naver_categories.json",
    backup: bool = True
) -> None:
    """
    í‚¤ì›Œë“œ ì €ì¥
    
    Args:
        keywords: í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬
        save_path: ì €ì¥ ê²½ë¡œ
        backup: ë°±ì—… ìƒì„± ì—¬ë¶€
    """
    save_path = Path(save_path)
    
    # ë°±ì—… ìƒì„±
    if backup and save_path.exists():
        import shutil
        from datetime import datetime
        backup_name = f"{save_path.stem}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        backup_path = save_path.parent / backup_name
        shutil.copy(save_path, backup_path)
        print(f"ğŸ“¦ ë°±ì—…: {backup_path}")
    
    # ì €ì¥
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(keywords, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì €ì¥: {save_path}")
    print(f"   ì´ {len(keywords)}ê°œ ì¹´í…Œê³ ë¦¬, {sum(len(v) for v in keywords.values())}ê°œ í‚¤ì›Œë“œ")


# ì¹´í…Œê³ ë¦¬ë³„ ì‹œë“œ ê²€ìƒ‰ì–´ (ë„¤ì´ë²„ ì‡¼í•‘ ê³µì‹ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜)
SEED_QUERIES = {
    "íŒ¨ì…˜ì˜ë¥˜": [
        "í‹°ì…”ì¸ ", "ì…”ì¸ ", "ë‹ˆíŠ¸", "ë§¨íˆ¬ë§¨", "í›„ë“œ", "ì½”íŠ¸", "ìì¼“", "ì²­ë°”ì§€"
    ],
    "íŒ¨ì…˜ì¡í™”": [
        "ê°€ë°©", "ì§€ê°‘", "ë²¨íŠ¸", "ëª¨ì", "ìŠ¤ì¹´í”„", "ì‹œê³„", "ì„ ê¸€ë¼ìŠ¤", "ëª©ê±¸ì´"
    ],
    "í™”ì¥í’ˆ/ë¯¸ìš©": [
        "í™”ì¥í’ˆ", "ìŠ¤í‚¨ì¼€ì–´", "ë©”ì´í¬ì—…", "ë¡œì…˜", "í¬ë¦¼", "ì—ì„¼ìŠ¤", "ë§ˆìŠ¤í¬íŒ©", "ì„ í¬ë¦¼"
    ],
    "ë””ì§€í„¸/ê°€ì „": [
        "ì´ì–´í°", "ë¬´ì„ ì¶©ì „ê¸°", "ë³´ì¡°ë°°í„°ë¦¬", "ìŠ¤ë§ˆíŠ¸ì›Œì¹˜", "íƒœë¸”ë¦¿", "í‚¤ë³´ë“œ", "ë§ˆìš°ìŠ¤"
    ],
    "ì¶œì‚°/ìœ¡ì•„": [
        "ê¸°ì €ê·€", "ë¶„ìœ ", "ìœ ëª¨ì°¨", "ì•„ê¸°ë ", "ìˆ˜ìœ ìš©í’ˆ", "ìœ ì•„ì‹", "ìœ¡ì•„ìš©í’ˆ"
    ],
    "ì‹í’ˆ": [
        "ê±´ê°•ì‹í’ˆ", "ë‹¤ì´ì–´íŠ¸ì‹í’ˆ", "ë‹¨ë°±ì§ˆ", "í”„ë¡œí‹´", "í™ì‚¼", "ì˜ì–‘ë°”", "ê±´ê³¼ë¥˜"
    ],
    "ìŠ¤í¬ì¸ /ë ˆì €": [
        "ìš´ë™ë³µ", "ìš´ë™í™”", "ìš”ê°€ë§¤íŠ¸", "ë¤ë²¨", "ëŸ°ë‹í™”", "ë“±ì‚°í™”", "ìì „ê±°"
    ],
    "ìƒí™œ/ê±´ê°•": [
        "ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ", "ë¹„íƒ€ë¯¼", "ì˜ì–‘ì œ", "í”„ë¡œë°”ì´ì˜¤í‹±ìŠ¤", "ì˜¤ë©”ê°€3", "ìœ ì‚°ê· ", "ê°ê¸°ì•½"
    ],
    "ì—¬ê°€/ìƒí™œí¸ì˜": [
        "ìº í•‘ìš©í’ˆ", "í…€ë¸”ëŸ¬", "ë³´ì˜¨ë³‘", "ìš°ì‚°", "ì—¬í–‰ê°€ë°©", "ìˆ˜ë‚©ìš©í’ˆ", "ìƒí™œìš©í’ˆ"
    ]
}


if __name__ == "__main__":
    import os
    
    CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "9LKTOG5R9F8Yx74PnZe0")
    CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET", "gytCGuuEeX")
    
    print("ğŸš€ ìë™ í‚¤ì›Œë“œ ë°œê²¬ ì‹œì‘\n")
    
    # 1. íŠ¸ë Œë”© í‚¤ì›Œë“œ ë°œê²¬
    discovered = discover_trending_keywords(
        client_id=CLIENT_ID,
        client_secret=CLIENT_SECRET,
        categories=SEED_QUERIES,
        max_keywords_per_category=50
    )
    
    print(f"\n{'='*70}")
    print("ğŸ“Š ë°œê²¬ ê²°ê³¼")
    print(f"{'='*70}")
    
    for category, keywords in discovered.items():
        print(f"\n{category}: {len(keywords)}ê°œ")
        print(f"  {', '.join(keywords[:10])}")
        if len(keywords) > 10:
            print(f"  ... ì™¸ {len(keywords)-10}ê°œ")
    
    # 2. ê¸°ì¡´ í‚¤ì›Œë“œì™€ ë³‘í•©
    print(f"\n{'='*70}")
    print("ğŸ”„ ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©")
    print(f"{'='*70}")
    
    # ê¸°ë³¸ê°’: replace (ìµœì‹  ì¸ê¸° í‚¤ì›Œë“œë§Œ ìœ ì§€)
    # "merge"ë¡œ ë³€ê²½í•˜ë©´ ê¸°ì¡´ í‚¤ì›Œë“œë„ ìœ ì§€
    merged = merge_with_existing_keywords(discovered, mode="replace")
    
    # 3. ì €ì¥
    print(f"\n{'='*70}")
    print("ğŸ’¾ ì €ì¥")
    print(f"{'='*70}\n")
    
    save_keywords(merged, backup=True)
    
    print(f"\n{'='*70}")
    print("âœ¨ ì™„ë£Œ!")
    print(f"{'='*70}")
    print("\nğŸ’¡ ì´ì œ Streamlit ì•±ì„ ë‹¤ì‹œ ì‹œì‘í•˜ë©´ ìƒˆë¡œìš´ í‚¤ì›Œë“œë¡œ ë¶„ì„ë©ë‹ˆë‹¤!")
    print("   streamlit run streamlit_app.py")

